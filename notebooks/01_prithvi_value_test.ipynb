{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# PyroSense: Wildfire Prediction with Foundation Models\n",
    "\n",
    "This notebook demonstrates the full PyroSense pipeline:\n",
    "1. **Data Loading**: Fire events from Mesogeos datacube\n",
    "2. **HLS Imagery**: Satellite composites from NASA Earthdata\n",
    "3. **Multi-Source Features**: Prithvi (1024-dim), Weather (10-dim), AlphaEarth (64-dim)\n",
    "4. **Stacking Ensemble**: Two-level architecture with source-specific base models\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  HLS Satellite  \u2502    \u2502  Weather Data   \u2502    \u2502   AlphaEarth    \u2502\n",
    "\u2502    Imagery      \u2502    \u2502  (Open-Meteo)   \u2502    \u2502 (Earth Engine)  \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "         \u2502                      \u2502                      \u2502\n",
    "         \u25bc                      \u25bc                      \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502    Prithvi      \u2502    \u2502    Weather      \u2502    \u2502   AlphaEarth    \u2502\n",
    "\u2502   Extractor     \u2502    \u2502   Extractor     \u2502    \u2502   Extractor     \u2502\n",
    "\u2502  (1024 dims)    \u2502    \u2502   (10 dims)     \u2502    \u2502   (64 dims)     \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "         \u2502                      \u2502                      \u2502\n",
    "         \u25bc                      \u25bc                      \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  Base Model 1   \u2502    \u2502  Base Model 2   \u2502    \u2502  Base Model 3   \u2502\n",
    "\u2502  (RandomForest) \u2502    \u2502  (RandomForest) \u2502    \u2502  (RandomForest) \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "         \u2502                      \u2502                      \u2502\n",
    "         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                                \u2502\n",
    "                                \u25bc\n",
    "                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "                    \u2502    Meta-Learner     \u2502\n",
    "                    \u2502 (LogisticRegression)\u2502\n",
    "                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                               \u2502\n",
    "                               \u25bc\n",
    "                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "                    \u2502   Fire Prediction   \u2502\n",
    "                    \u2502   P(fire | event)   \u2502\n",
    "                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress verbose logging and warnings\n",
    "from loguru import logger\n",
    "logger.disable(\"pyrosense\")  # Disable pyrosense logs in notebook\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)  # Suppress sklearn warnings\n",
    "\n",
    "# PyroSense imports\n",
    "from pyrosense.data.mesogeos_loader import MesogeosLoader, save_events_csv, load_events_csv\n",
    "from pyrosense.data.hls_downloader import HLSDownloader\n",
    "from pyrosense.features.prithvi import PrithviExtractor\n",
    "from pyrosense.features.weather import WeatherExtractor\n",
    "from pyrosense.features.store import FeatureStore\n",
    "from pyrosense.models.stacking import StackingEnsemble, StackingConfig\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"../data\")\n",
    "MESOGEOS_PATH = DATA_DIR / \"mesogeos/mesogeos.zarr\"\n",
    "HLS_DIR = DATA_DIR / \"hls\"\n",
    "FEATURE_STORE_DIR = DATA_DIR / \"features\"\n",
    "EVENTS_CACHE = DATA_DIR / \"fire_events.csv\"\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Load Fire Events from Mesogeos\n",
    "\n",
    "Mesogeos is used **only** for fire labels (where/when fires occurred).\n",
    "The predictive features come from Prithvi, Weather, and AlphaEarth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or extract fire events\n",
    "if EVENTS_CACHE.exists():\n",
    "    all_events = load_events_csv(str(EVENTS_CACHE))\n",
    "    fire_events = [e for e in all_events if e.burned_area > 0]\n",
    "    negative_events = [e for e in all_events if e.burned_area == 0]\n",
    "    print(f\"Loaded from cache: {len(fire_events)} fire + {len(negative_events)} no-fire\")\n",
    "else:\n",
    "    loader = MesogeosLoader(str(MESOGEOS_PATH), region=\"greece\")\n",
    "    fire_events = loader.extract_fire_events(\n",
    "        n_samples=100, min_burned_area=0.0,\n",
    "        start_year=2015, end_year=2021, random_seed=42,\n",
    "    )\n",
    "    # Sample negative events from DIFFERENT locations (no spatial leakage)\n",
    "    negative_events = loader.sample_negative_events_different_locations(\n",
    "        n_samples=100,\n",
    "        fire_events=fire_events,\n",
    "        min_distance_deg=0.1,  # ~10km minimum from any fire\n",
    "        random_seed=42,\n",
    "    )\n",
    "    all_events = fire_events + negative_events\n",
    "    save_events_csv(all_events, str(EVENTS_CACHE))\n",
    "    print(f\"Extracted: {len(fire_events)} fire + {len(negative_events)} no-fire\")\n",
    "\n",
    "all_events = fire_events + negative_events\n",
    "print(f\"Total events: {len(all_events)}\")\n",
    "\n",
    "# Create labels\n",
    "labels = {e.event_id: 1.0 if e.burned_area > 0 else 0.0 for e in all_events}\n",
    "y = np.array([labels[e.event_id] for e in all_events])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize fire event locations\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "fire_lats = [e.latitude for e in fire_events]\n",
    "fire_lons = [e.longitude for e in fire_events]\n",
    "neg_lats = [e.latitude for e in negative_events]\n",
    "neg_lons = [e.longitude for e in negative_events]\n",
    "\n",
    "# No jitter needed - locations are now different\n",
    "ax.scatter(neg_lons, neg_lats, c='steelblue', alpha=0.6, s=40, \n",
    "           marker='s', label='No fire', edgecolors='white', linewidth=0.5)\n",
    "ax.scatter(fire_lons, fire_lats, c='red', alpha=0.8, s=50, \n",
    "           marker='o', label='Fire', edgecolors='white', linewidth=0.5)\n",
    "\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_title('Fire Event Locations (Greece)\\nNegative events sampled from different locations (no spatial leakage)')\n",
    "ax.legend()\n",
    "ax.set_xlim(19, 30)\n",
    "ax.set_ylim(34.5, 42)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Download HLS Satellite Imagery\n",
    "\n",
    "For each event, we download a 6-band HLS composite from NASA Earthdata.\n",
    "The 6 bands (Blue, Green, Red, NIR, SWIR1, SWIR2) are what Prithvi expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip downloading - just use what's already on disk\n",
    "SKIP_HLS_DOWNLOAD = True  # Set to False to download missing images\n",
    "\n",
    "downloader = HLSDownloader(\n",
    "    output_dir=str(HLS_DIR),\n",
    "    days_before=30,\n",
    "    min_days_before=7,\n",
    ")\n",
    "\n",
    "if SKIP_HLS_DOWNLOAD:\n",
    "    print(\"Skipping HLS download - using cached composites only\")\n",
    "    composites = downloader.get_available_composites()\n",
    "    print(f\"Found {len(composites)} cached HLS composites\")\n",
    "    \n",
    "    # Count how many events have imagery\n",
    "    fire_with_hls = sum(1 for e in fire_events if e.event_id in composites)\n",
    "    nofire_with_hls = sum(1 for e in negative_events if e.event_id in composites)\n",
    "    print(f\"  Fire events with HLS: {fire_with_hls}/{len(fire_events)}\")\n",
    "    print(f\"  No-fire events with HLS: {nofire_with_hls}/{len(negative_events)}\")\n",
    "else:\n",
    "    print(\"Downloading HLS imagery (7-30 days BEFORE each event)\")\n",
    "    print(\"=\" * 60)\n",
    "    successes, failures = downloader.download_for_events(all_events)\n",
    "    print(f\"\\nDownload complete: {len(successes)}/{len(all_events)} succeeded\")\n",
    "    composites = downloader.get_available_composites()\n",
    "\n",
    "print(f\"\\nTotal available composites: {len(composites)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": "## 4. Feature Extraction with FeatureStore\n\nWe extract features from multiple sources and store them using the project's FeatureStore."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the feature store\n",
    "store = FeatureStore(str(FEATURE_STORE_DIR))\n",
    "\n",
    "print(f\"Feature store: {FEATURE_STORE_DIR}\")\n",
    "print(f\"Existing sources: {store.list_sources()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### 4.1 Prithvi Features (1024-dim)\n",
    "\n",
    "Extract features from frozen Prithvi-EO-2.0-300M encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "if store.exists(\"prithvi\"):\n",
    "    prithvi_df = store.load(\"prithvi\")\n",
    "    print(f\"Loaded Prithvi features from store: {prithvi_df.shape}\")\n",
    "else:\n",
    "    print(\"Extracting Prithvi features...\")\n",
    "    prithvi_extractor = PrithviExtractor(model_name=\"Prithvi-EO-2.0-300M\", device=\"auto\")\n",
    "    print(f\"Device: {prithvi_extractor.device}\")\n",
    "    \n",
    "    # Get available composites\n",
    "    composites = downloader.get_available_composites()\n",
    "    print(f\"Found {len(composites)} HLS composites\")\n",
    "    \n",
    "    # Build image paths for events with available composites\n",
    "    image_paths = []\n",
    "    events_with_images = []\n",
    "    for event in all_events:\n",
    "        composite = composites.get(event.event_id)\n",
    "        if composite and composite.exists():\n",
    "            image_paths.append(composite)\n",
    "            events_with_images.append(event)\n",
    "    \n",
    "    print(f\"Events with HLS imagery: {len(events_with_images)}/{len(all_events)}\")\n",
    "    \n",
    "    # Extract features (returns DataFrame with prithvi_ prefix already)\n",
    "    prithvi_df = prithvi_extractor.extract_batch(events_with_images, image_paths=image_paths)\n",
    "    \n",
    "    # Save to store\n",
    "    store.save(\"prithvi\", prithvi_df)\n",
    "    print(f\"Extracted and saved: {prithvi_df.shape}\")\n",
    "\n",
    "print(f\"Prithvi features: {prithvi_df.shape[1]} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "### 4.2 Weather Features (10-dim)\n",
    "\n",
    "Extract 7-day pre-event weather from Open-Meteo API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "if store.exists(\"weather\"):\n",
    "    weather_df = store.load(\"weather\")\n",
    "    print(f\"Loaded Weather features from store: {weather_df.shape}\")\n",
    "else:\n",
    "    print(\"Extracting Weather features...\")\n",
    "    weather_extractor = WeatherExtractor(days_before=7)\n",
    "    \n",
    "    # extract_batch returns DataFrame with weather_ prefix already\n",
    "    weather_df = weather_extractor.extract_batch(all_events)\n",
    "    \n",
    "    # Save to store\n",
    "    store.save(\"weather\", weather_df)\n",
    "    print(f\"Extracted and saved: {weather_df.shape}\")\n",
    "\n",
    "print(f\"Weather features: {weather_df.shape[1]} dimensions\")\n",
    "print(f\"Feature names: {list(weather_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "### 4.3 AlphaEarth Features (64-dim) - Optional\n",
    "\n",
    "Extract land surface embeddings from Google Earth Engine.\n",
    "\n",
    "**Note:** Requires Earth Engine authentication. Skip if not configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "import os\n\n# AlphaEarth - requires Earth Engine authentication\nUSE_ALPHAEARTH = True\n\n# Set your GCP project ID here or via environment variable GOOGLE_CLOUD_PROJECT\nGCP_PROJECT = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", None) or \"your-project-id\"\n\nif USE_ALPHAEARTH:\n    if not GCP_PROJECT:\n        print(\"AlphaEarth requires a GCP project ID.\")\n        print(\"   Set via: export GOOGLE_CLOUD_PROJECT=your-project-id\")\n        print(\"   Skipping AlphaEarth features...\")\n        alphaearth_df = None\n    else:\n        from pyrosense.features.alphaearth import AlphaEarthExtractor\n        \n        if store.exists(\"alphaearth\"):\n            alphaearth_df = store.load(\"alphaearth\")\n            print(f\"Loaded AlphaEarth features from store: {alphaearth_df.shape}\")\n        else:\n            print(f\"Extracting AlphaEarth features (project: {GCP_PROJECT})...\")\n            alphaearth_extractor = AlphaEarthExtractor(\n                cache_dir=str(DATA_DIR / \"alphaearth\"),\n                project=GCP_PROJECT\n            )\n            \n            # extract_batch returns DataFrame with alphaearth_ prefix already\n            alphaearth_df = alphaearth_extractor.extract_batch(all_events)\n            \n            store.save(\"alphaearth\", alphaearth_df)\n            print(f\"Extracted and saved: {alphaearth_df.shape}\")\n        \n        print(f\"AlphaEarth features: {alphaearth_df.shape[1]} dimensions\")\nelse:\n    print(\"AlphaEarth disabled.\")\n    alphaearth_df = None"
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "### 4.4 Combine Features\n",
    "\n",
    "Merge all feature sources into a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine available features\n",
    "sources = [\"prithvi\", \"weather\"]\n",
    "if alphaearth_df is not None:\n",
    "    sources.append(\"alphaearth\")\n",
    "\n",
    "X_combined = store.get_combined(sources)\n",
    "\n",
    "# Get events that have all features (primarily limited by Prithvi/HLS availability)\n",
    "available_events = [e for e in all_events if e.event_id in X_combined.index]\n",
    "print(f\"Events with all features: {len(available_events)}/{len(all_events)}\")\n",
    "\n",
    "# Align with available events\n",
    "event_ids = [e.event_id for e in available_events]\n",
    "X_combined = X_combined.reindex(event_ids)\n",
    "\n",
    "# Update labels for available events\n",
    "y_aligned = np.array([labels[eid] for eid in X_combined.index])\n",
    "\n",
    "print(f\"Combined features shape: {X_combined.shape}\")\n",
    "print(f\"Sources: {sources}\")\n",
    "print(f\"\\nFeature breakdown:\")\n",
    "for source in sources:\n",
    "    cols = [c for c in X_combined.columns if c.startswith(f\"{source}_\")]\n",
    "    print(f\"  {source}: {len(cols)} dimensions\")\n",
    "    \n",
    "print(f\"\\nClass balance: {y_aligned.mean():.1%} fire, {1-y_aligned.mean():.1%} no-fire\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 5. Train Stacking Ensemble\n",
    "\n",
    "The stacking ensemble uses:\n",
    "- **Level 1 (Base models)**: One RandomForest per feature source\n",
    "- **Level 2 (Meta-learner)**: LogisticRegression on base model predictions\n",
    "\n",
    "This allows each source to be modeled independently, then combined optimally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# y_aligned is already created in the previous cell\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_combined, y_aligned, test_size=0.2, random_state=42, stratify=y_aligned\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]} samples\")\n",
    "print(f\"Test:  {X_test.shape[0]} samples\")\n",
    "print(f\"Class balance (train): {y_train.mean():.1%} fire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure stacking ensemble\n",
    "config = StackingConfig.simple(sources=sources)\n",
    "\n",
    "print(\"Stacking Configuration:\")\n",
    "print(f\"  CV folds: {config.cv_folds}\")\n",
    "print(f\"  Use probabilities: {config.use_probabilities}\")\n",
    "print(f\"\\nFeature groups:\")\n",
    "for group in config.feature_groups:\n",
    "    print(f\"  - {group.name}: prefix='{group.column_prefix}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the stacking ensemble\n",
    "ensemble = StackingEnsemble(config)\n",
    "\n",
    "print(\"Training stacking ensemble...\")\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(ensemble.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve, accuracy_score, \n",
    "    precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "y_prob = ensemble.predict_proba(X_test)[:, 1]\n",
    "y_pred = ensemble.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "print(\"=\" * 50)\n",
    "print(\"STACKING ENSEMBLE - Test Set Metrics\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  AUC:       {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "print(f\"  Accuracy:  {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_pred, zero_division=0):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_test, y_pred, zero_division=0):.4f}\")\n",
    "print(f\"  F1:        {f1_score(y_test, y_pred, zero_division=0):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source contribution weights\n",
    "weights = ensemble.source_weights()\n",
    "\n",
    "print(\"\\nSource Contribution Weights:\")\n",
    "for source, weight in sorted(weights.items(), key=lambda x: -x[1]):\n",
    "    bar = \"\u2588\" * int(weight * 40)\n",
    "    print(f\"  {source:12s}: {weight:.3f} {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC\n",
    "ax = axes[0]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "auc_val = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "ax.plot(fpr, tpr, linewidth=2, label=f'Stacking Ensemble (AUC = {auc_val:.3f})')\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random (AUC = 0.500)')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curve \u2014 Stacking Ensemble')\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Confusion Matrix\n",
    "ax = axes[1]\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=['No Fire', 'Fire'])\n",
    "disp.plot(ax=ax, cmap='Blues')\n",
    "ax.set_title('Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 7. Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "\n",
    "# Re-train on full data for CV\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Create a fresh ensemble for CV\n",
    "cv_ensemble = StackingEnsemble(config)\n",
    "\n",
    "# Use custom scorer to ensure predict_proba is used correctly\n",
    "def auc_scorer(estimator, X, y):\n",
    "    y_prob = estimator.predict_proba(X)[:, 1]\n",
    "    return roc_auc_score(y, y_prob)\n",
    "\n",
    "cv_scores = cross_val_score(\n",
    "    cv_ensemble, X_combined, y_aligned, \n",
    "    cv=cv, scoring=auc_scorer\n",
    ")\n",
    "\n",
    "print(f\"5-Fold CV AUC: {cv_scores.mean():.4f} \u00b1 {cv_scores.std():.4f}\")\n",
    "print(f\"  Per fold: {[f'{s:.3f}' for s in cv_scores]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## 8. Comparison: Stacking vs Single Source\n",
    "\n",
    "Compare the stacking ensemble against using only one feature source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "results = []\n",
    "\n",
    "# Test each source individually\n",
    "for source in sources:\n",
    "    # Get columns for this source\n",
    "    cols = [c for c in X_combined.columns if c.startswith(f\"{source}_\")]\n",
    "    X_source = X_combined[cols]\n",
    "    \n",
    "    # Scale and clean\n",
    "    X_clean = np.nan_to_num(X_source.values, nan=0.0)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_clean)\n",
    "    \n",
    "    # CV\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=15, random_state=42)\n",
    "    scores = cross_val_score(clf, X_scaled, y_aligned, cv=cv, scoring=\"roc_auc\")\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": f\"{source.capitalize()} only\",\n",
    "        \"Features\": len(cols),\n",
    "        \"CV AUC\": scores.mean(),\n",
    "        \"Std\": scores.std()\n",
    "    })\n",
    "\n",
    "# Add stacking result\n",
    "results.append({\n",
    "    \"Model\": \"Stacking Ensemble\",\n",
    "    \"Features\": X_combined.shape[1],\n",
    "    \"CV AUC\": cv_scores.mean(),\n",
    "    \"Std\": cv_scores.std()\n",
    "})\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results).sort_values(\"CV AUC\", ascending=False)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Color the Stacking Ensemble differently (it's always the last row before sorting)\n",
    "# After sorting, we need to identify it by name\n",
    "colors = ['coral' if 'Stacking' in model else 'steelblue' for model in results_df[\"Model\"]]\n",
    "\n",
    "bars = ax.barh(results_df[\"Model\"], results_df[\"CV AUC\"], color=colors, alpha=0.8)\n",
    "ax.errorbar(\n",
    "    results_df[\"CV AUC\"], results_df[\"Model\"], \n",
    "    xerr=results_df[\"Std\"], fmt='none', color='black', capsize=5\n",
    ")\n",
    "\n",
    "ax.set_xlabel('CV AUC')\n",
    "ax.set_title('Model Comparison: Stacking vs Single Source')\n",
    "ax.set_xlim(0, 1.05)\n",
    "ax.axvline(x=0.5, color='gray', linestyle='--', alpha=0.5, label='Random baseline')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, results_df[\"CV AUC\"]):\n",
    "    ax.text(val + 0.02, bar.get_y() + bar.get_height()/2, f'{val:.3f}', \n",
    "            va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## 9. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "MODEL_PATH = DATA_DIR / \"stacking_ensemble.pkl\"\n",
    "\n",
    "# Retrain on full data\n",
    "final_ensemble = StackingEnsemble(config)\n",
    "final_ensemble.fit(X_combined, y_aligned)\n",
    "\n",
    "# Save\n",
    "joblib.dump(final_ensemble, MODEL_PATH)\n",
    "print(f\"Model saved to: {MODEL_PATH}\")\n",
    "\n",
    "# Verify\n",
    "loaded = joblib.load(MODEL_PATH)\n",
    "test_pred = loaded.predict_proba(X_test)[:, 1]\n",
    "print(f\"Verification AUC: {roc_auc_score(y_test, test_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the full PyroSense pipeline:\n",
    "\n",
    "1. **Data**: Loaded fire events from Mesogeos datacube\n",
    "2. **Features**: Extracted multi-source features:\n",
    "   - Prithvi (1024-dim): Satellite imagery embeddings\n",
    "   - Weather (10-dim): 7-day pre-event conditions\n",
    "   - AlphaEarth (64-dim): Land surface embeddings (optional)\n",
    "3. **Model**: Trained stacking ensemble with source-specific base models\n",
    "4. **Evaluation**: Compared against single-source baselines\n",
    "5. **VLM Analysis**: EarthDial for interactive fire area Q&A (optional)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Enable AlphaEarth features for improved predictions\n",
    "- Increase dataset size (more fire events)\n",
    "- Test on different geographic regions\n",
    "- Use EarthDial for detailed fire risk reports\n",
    "- Deploy with CLI:\n",
    "  - `pyrosense predict --model stacking_ensemble.pkl --lat 38.5 --lon 23.1 --date 2024-07-15`\n",
    "  - `pyrosense analyze --image data/hls/fire_0001/composite.tif`\n",
    "  - `pyrosense chat --image data/hls/fire_0001/composite.tif`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grn897gw536",
   "metadata": {},
   "source": [
    "## 10. EarthDial VLM Analysis (Optional)\n",
    "\n",
    "Use EarthDial Vision-Language Model to interactively analyze high-risk areas.\n",
    "\n",
    "**Requirements:**\n",
    "- Install: `pip install pyrosense[earthdial]`\n",
    "- ~8GB VRAM/RAM for float16 model\n",
    "- First run downloads ~8GB model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hlj2ior1jqk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: EarthDial analysis of high-risk areas\n",
    "# Requires: pip install pyrosense[earthdial]\n",
    "\n",
    "ENABLE_EARTHDIAL = True  # Set to True to enable (requires ~8GB memory)\n",
    "\n",
    "if ENABLE_EARTHDIAL:\n",
    "    try:\n",
    "        import warnings\n",
    "        warnings.filterwarnings(\"ignore\")  # Suppress all warnings for cleaner output\n",
    "        \n",
    "        from pyrosense.vlm import EarthDialAssistant, hls_to_rgb\n",
    "        \n",
    "        # Find highest risk prediction from test set\n",
    "        high_risk_idx = y_prob.argmax()\n",
    "        high_risk_event_id = X_test.index[high_risk_idx]\n",
    "        high_risk_event = next(e for e in available_events if e.event_id == high_risk_event_id)\n",
    "        high_risk_prob = y_prob[high_risk_idx]\n",
    "        \n",
    "        print(f\"Analyzing highest-risk area:\")\n",
    "        print(f\"  Event: {high_risk_event.event_id}\")\n",
    "        print(f\"  Location: ({high_risk_event.latitude:.4f}, {high_risk_event.longitude:.4f})\")\n",
    "        print(f\"  Fire probability: {high_risk_prob:.1%}\")\n",
    "        \n",
    "        # Get HLS image path\n",
    "        image_path = HLS_DIR / high_risk_event.event_id / \"composite.tif\"\n",
    "        \n",
    "        if image_path.exists():\n",
    "            # Convert to RGB with cropping centered on event location\n",
    "            # Crops to 224x224 pixels (6.7 km x 6.7 km) - same area used by Prithvi\n",
    "            rgb_path = hls_to_rgb(\n",
    "                image_path,\n",
    "                center_lat=high_risk_event.latitude,\n",
    "                center_lon=high_risk_event.longitude,\n",
    "                crop_size=224\n",
    "            )\n",
    "            \n",
    "            # Display the CROPPED image (same area as fire probability prediction)\n",
    "            from PIL import Image\n",
    "            img = Image.open(rgb_path)\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"High-Risk Area: {high_risk_event.event_id}\\n\"\n",
    "                     f\"Fire Probability: {high_risk_prob:.1%}\\n\"\n",
    "                     f\"Area: 6.7 km \u00d7 6.7 km (224\u00d7224 pixels @ 30m)\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "            # Load EarthDial and analyze\n",
    "            print(\"\\nLoading EarthDial model...\")\n",
    "            assistant = EarthDialAssistant(device=\"auto\")\n",
    "            \n",
    "            # Generate report (uses event coordinates for consistent cropping)\n",
    "            print(\"Generating fire risk report...\")\n",
    "            report = assistant.generate_report(\n",
    "                image_path,\n",
    "                event=high_risk_event,\n",
    "                fire_probability=high_risk_prob\n",
    "            )\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"EARTHDIAL FIRE RISK REPORT\")\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"\\nSUMMARY:\\n{report['summary']}\")\n",
    "            print(f\"\\nVEGETATION:\\n{report['vegetation_analysis']}\")\n",
    "            print(f\"\\nTERRAIN:\\n{report['terrain_factors']}\")\n",
    "            print(f\"\\nSTRATEGIES:\\n{report['recommended_strategies']}\")\n",
    "            print(f\"\\nRISK ASSESSMENT:\\n{report['risk_assessment']}\")\n",
    "            print(\"=\" * 60)\n",
    "        else:\n",
    "            print(f\"HLS image not found: {image_path}\")\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"EarthDial not installed.\")\n",
    "        print(\"Install with: pip install pyrosense[earthdial]\")\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"EarthDial error: {e}\")\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"EarthDial analysis disabled. Set ENABLE_EARTHDIAL = True to enable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77416c1-6e56-4fca-8aaa-7dd88f79973b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Wildfire Prithvi",
   "language": "python",
   "name": "wildfire-prithvi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}